# Degradation Attacks on Certifiably Robust Neural Networks
This repository contains code for the experiments in the  paper "Degradation Attacks on Certifiably Robust Neural Networks". 

## What is the problem with certifiably robust neural networks?
Certifiably robust neural networks protect against adversarial examples by employing provable run-time defenses that check if the model is locally robust at the input under evaluation. We show through examples and experiments that even complete defenses are inherently over-cautious. 
Specifically, they flag inputs for which local robustness checks fail, but yet that are not adversarial; 
i.e., they are classified consistently with all valid inputs within a distance of Îµ. 
As a result, while a norm-bounded adversary cannot change the classification of an input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. 

## How to reproduce the results in the paper?

### Get started 

1. Clone the repository

2. Setup a conda environment and install the relevant packages as follows
```
conda create --name degrade
conda activate degrade
conda install -c conda-forge tensorflow 
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
pip install cachable scriptify tensorflow_datasets matplotlib pandas seaborn foolbox
```
Note that the gloro folder contains the GloRo implementation by Leino et al. copied from [here](https://github.com/klasleino/gloro). We make a small modification to the code so that a GloRo model returns the certified radius along with its prediction. 


### Upper Bound experiments
In these experiments, we compute upper bounds on the efficacy of degradation attacks, i.e., upper bounds on the false positive rates. 
For GloRo, we train our own models (`trainGloro.py`) using the same hyperparameters as used by [Leino et al.](https://arxiv.org/abs/2102.08452). Our trained models are available in the `models` folder. 
For each trained GloRo model, we generate data files (`printRadius.py`) with one row for every sample in the test dataset of the form `(index , correct label, correct label, certified radius, is correct?)`. The pre-generated evaluation results are saved in the `data` folder.  
For the Randomized Smoothed models, we use the data files generated by Cohen et al. available [here](https://github.com/locuslab/smoothing/tree/master/data/certify). A copy of these files is also available in the `data` folder in our repository. 
For generating the plots in the paper, we use the file `analyze.py`.

#### experiments/trainGloro.py

This file trains the GloRo models. We used the following commands and hyperprameters for training the models. The trained models are available in the `models/gloro` folder. Saved models with suffix `_Y` were trained using double the required radius whereas models with suffix `_N` were only trained using the required radius.
```
python experiments/trainGloro.py --superrobust=Y --dataset=mnist --architecture=minmax_cnn_2C2F --epsilon=0.3 --epsilon_train=0.3 --epsilon_schedule=fixed --loss=sparse_trades_kl.2.0 --augmentation=None --epochs=500 --batch_size=128 --optimizer=adam --lr=1e-3 --lr_schedule=decay_to_0.000001 --trades_schedule=linear_from_0.1

python experiments/trainGloro.py --superrobust=N --dataset=mnist --architecture=minmax_cnn_2C2F --epsilon=0.3 --epsilon_train=0.3 --epsilon_schedule=fixed --loss=sparse_trades_kl.0.1 --augmentation=None --epochs=500 --batch_size=128 --optimizer=adam --lr=1e-3 --lr_schedule=decay_to_0.000001 --trades_schedule=linear_from_0.1

python experiments/trainGloro.py --superrobust=Y --dataset=mnist --architecture=minmax_cnn_4C3F --epsilon=1.58 --epsilon_train=1.74 --epsilon_schedule=logarithmic --loss=sparse_trades_kl.1.5 --augmentation=None --epochs=500 --batch_size=128 --optimizer=adam --lr=1e-3 --lr_schedule=decay_to_0.000005 --trades_schedule=fixed

python experiments/trainGloro.py --superrobust=N --dataset=mnist --architecture=minmax_cnn_4C3F --epsilon=1.58 --epsilon_train=1.74 --epsilon_schedule=logarithmic --loss=sparse_trades_kl.1.5 --augmentation=None --epochs=500 --batch_size=128 --optimizer=adam --lr=1e-3 --lr_schedule=decay_to_0.000005 --trades_schedule=fixed

python experiments/trainGloro.py --superrobust=Y --dataset=cifar10 --architecture=minmax_cnn_6C2F --epsilon=0.141 --epsilon_train=0.141 --epsilon_schedule=logarithmic --loss=sparse_trades_kl.1.2 --augmentation=cifar --epochs=800 --batch_size=512 --optimizer=adam --lr=1e-3 --lr_schedule=decay_to_0.000001 --trades_schedule=fixed

python experiments/trainGloro.py --superrobust=N --dataset=cifar10 --architecture=minmax_cnn_6C2F --epsilon=0.141 --epsilon_train=0.141 --epsilon_schedule=logarithmic --loss=sparse_trades_kl.1.2 --augmentation=cifar --epochs=800 --batch_size=512 --optimizer=adam --lr=1e-3 --lr_schedule=decay_to_0.000001 --trades_schedule=fixed
```

#### experiments/upper_bounds/printRadius.py
This file evaluates the GloRo models and prints out the correct label, model predicted label, certified robust radius and correctness for each test data point in the given dataset.
The generated files are available in the `data/gloro` folder. An example command for running `printRadius` is as follows.

```
python experiments/upper_bounds/printRadius.py --dataset=cifar10 --batch_size=128 --model=cifar10_0.14_N
```


#### experiments/upper_bounds/analyze.py
This file is used to generate the plots in the paper. It uses the data stored in the `data` folder.

```
python experiments/upper_bounds/analyze.py
```

### Lower Bound experiments
In these experiments, we compute lower bounds on the efficacy of the degradation attacks using our attack algorithms. The attack results are printed on the command line.

#### experiments/dos/script_attack_gloro.py
This contains the code for attacking the GloRo models.
We use the models trained by us available in the folder `models/gloro`.
An example command for running the attack is as follows.
```
python experiments/lower_bounds/script_attack_gloro.py --dataset=cifar10 --model_path=./models/gloro/cifar10/cifar10_0.14_N --epsilon=0.141 

```

#### experiments/dos/script_attack_rs.py
This contains the code for attacking the Randomized Smoothed models. 
We use the pre-trained models generated by Cohen et al. available [here](https://drive.google.com/file/d/1h_TpbXm5haY5f-l4--IKylmdz6tvPoR4/view?usp=sharing).
An example command for running the attack is as follows.
```
python experiments/lower_bounds/script_attack_rs.py --dataset=cifar10 --model_path=<path_to_RS_model> --epsilon=0.5 --sigma=0.25 

```
Note that the trained RS model used should be the one trained with noise level corresponding to sigma.
 
#### experiments/dos/script_attack_kw.py 
This contains the code for calculating the attack efficacy lower bounds as well upper bounds for the KW models.
We use the pre-trained models generated by Wong et al. available [here](https://github.com/locuslab/convex_adversarial/tree/master/models_scaled).
An example command for running the attack is as follows.
```
python experiments/lower_bounds/script_attack_kw.py --dataset=cifar10 --model_path=<path_to_KW_model> --architecture=large_color --epsilon=2/255 

```
Note that we use `mnist_large_0_1.pth` and `cifar_large_2px.pth` KW models in our experiments. For MNIST, `architecture` should be set to `large`.

## Contributors
Chi Zhang, Klas Leino, and Ravi Mangal
