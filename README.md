# Degradation Attacks on Certifiably Robust Neural Networks
This repository contains code for the paper "Degradation Attacks on Certifiably Robust Neural Networks". 

## What's happening to the certifiably robust neural networks?
Certifiably robust neural networks protect against adversarial examples by employing provable run-time defenses that check if the model is locally robust at the input under evaluation. We show through examples and experiments that even complete defenses are inherently over-cautious. 
Specifically, they flag inputs for which local robustness checks fail, but yet that are not adversarial; 
i.e., they are classified consistently with all valid inputs within a distance of Îµ. 
As a result, while a norm-bounded adversary cannot change the classification of an input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. 

## How to reproduce the results in the paper?

### Get started 

1. Clone the repository and change into its root directory.

2. Install from source via
```
conda env create -f experimants/fpr/environment.yml
conda activate myenv
pip install -e .
```

### Lowerbound experiments
In this experiments, we compute lower bounds on the efficacy of the degradation attacks using the smoothed projected gradient descent attack (SPGD) algorithm. 
File `experiments/dos/script_attack_kw.py` calculate the attack results on gloro model and file `experiments/dos/script_attack_rs.py` calculate the results on randomized smoothing model. 

### Upperbound experiments
In this experiments, we compute upper bounds on the efficacy of degradation attacks, i.e., upper bounds on the false positive rates. 
The experiment includes both gloro model and randomized smoothing model. 
In gloro, we train our own model (`trainGloro.py`) and calculate the certified radius (`printRadius.py`), to get the data and in randomized smoothing model, we use the data generated by randomized smoothing paper at `https://github.com/locuslab/smoothing/tree/master/data/certify`. 
For data analyzing and generate the plots shown on the paper, we use the file analyze.py.

#### experiments/fpr/trainGloro.py
This file trains the gloro models. Hyperprameters for trainning and evaluating the model should be provided in the command line as following. 
```
python trainGloro.py --experiment='gloro' --superrobust='Y' --dataset="mnist" --architecture="minmax_cnn_2C2F" --epsilon=0.3 --epsilon_train=0.3 --epsilon_schedule='fixed' --loss='trades.0.1' --augmentation=None --epochs=500 --batch_size=128 --optimizer='adam' --lr=1e-3 --lr_schedule='decay_to_0.000001' --trades_schedule='linear,0.1,2.0'

python trainGloro.py --experiment='gloro' --superrobust='N' --dataset="mnist" --architecture="minmax_cnn_2C2F" --epsilon=0.3 --epsilon_train=0.3 --epsilon_schedule='fixed' --loss='trades.0.1' --augmentation=None --epochs=500 --batch_size=128 --optimizer='adam' --lr=1e-3 --lr_schedule='decay_to_0.000001' --trades_schedule='linear,0.1,2.0'

python trainGloro.py --experiment='gloro' --superrobust='Y' --dataset="mnist" --architecture="minmax_cnn_4C3F" --epsilon=1.58 --epsilon_train=1.74 --epsilon_schedule='logarithmic' --loss='trades.1.5' --augmentation=None --epochs=500 --batch_size=128 --optimizer='adam' --lr=1e-3 --lr_schedule='decay_to_0.000005' --trades_schedule='fixed'

python trainGloro.py --experiment='gloro' --superrobust='N' --dataset="mnist" --architecture="minmax_cnn_4C3F" --epsilon=1.58 --epsilon_train=1.74 --epsilon_schedule='logarithmic' --loss='trades.1.5' --augmentation=None --epochs=500 --batch_size=128 --optimizer='adam' --lr=1e-3 --lr_schedule='decay_to_0.000005' --trades_schedule='fixed'

python trainGloro.py --experiment='gloro' --superrobust='Y' --dataset="cifar10" --architecture="minmax_cnn_6C2F" --epsilon=36/255 --epsilon_train=36/255 --epsilon_schedule='logarithmic' --loss='trades.1.2' --augmentation='cifar' --epochs=800 --batch_size=512 --optimizer='adam' --lr=1e-3 --lr_schedule='decay_to_0.000001' --trades_schedule='fixed'

python trainGloro.py --experiment='gloro' --superrobust='N' --dataset="cifar10" --architecture="minmax_cnn_6C2F" --epsilon=36/255 --epsilon_train=36/255 --epsilon_schedule='logarithmic' --loss='trades.1.2' --augmentation='cifar' --epochs=800 --batch_size=512 --optimizer='adam' --lr=1e-3 --lr_schedule='decay_to_0.000001' --trades_schedule='fixed'
```

#### experiments/fpr/printRadius.py
Print out the correct label, model predicted label, certified robust radius and correctness for each point of test data in the given dataset, using the given gloro models. 
The input model could either be the model generated by file trainGloro.py or model in directory models. 
An example command to run this file is shown as following.

```
python experiments/fpr/printRadius.py --dataset="cifar10" --batch_size=128 --model="./models/cifar10_0.14_N.gloronet"
```


#### experiments/fpr/analyze.py
Analyze the data of certified radius etc. and generate plots for visualization. 
The data could from both gloro model (by file `printRadius.py`) and randomized smoothing model. 
The generated data for both models are stored in the file `experimants/fpr/data/`

```
python experiments/fpr/analyze.py --dataFile=experimants/fpr/
```
